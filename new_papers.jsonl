{"title": "ContraSUM: Party-Specific Summarization of Important Obligations, Entitlements, and Prohibitions in Legal Documents", "authors": ["Abhilasha Sancheti", "Aparna Garimella", "Balaji Vasan Srinivasan", "Rachel Rudinger"], "year": 2022, "venue": "arXiv preprint arXiv:2212.09825", "url": "https://arxiv.org/abs/2212.09825", "problem": "Legal contracts are long and written in legalese, requiring manual hours to understand obligations, entitlements, and prohibitions for each party", "prior_assumptions": "Generic contract summarization sufficient; no need for party-specific or commitment-type analysis", "insight": "Party-specific summarization categorizing obligations, entitlements, and prohibitions provides more useful analysis than generic approaches", "technical_approach": "Two-module system: (1) content categorizer to identify sentences containing each category for a party, (2) importance ranker using ~293K expert annotations to produce ranked lists", "evaluation": "Automatic and human evaluation comparing against text ranking baselines on lease agreements", "impact": "First party-specific contract analysis system with structured commitment categorization", "relevance_to_our_work": "Most similar to our landlord-tenant focus, demonstrates expert annotation methodology for contract analysis", "assumptions_made": ["Legal contracts can be categorized into obligations, entitlements, and prohibitions", "Pairwise importance comparisons by experts can train ranking systems", "Party-specific analysis more useful than generic summaries"], "bit_flip_potential": "Challenges assumption that generic contract analysis sufficient - shows need for party-specific structured analysis"}

{"title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "authors": ["Andreas Plesner", "et al."], "year": 2025, "venue": "arXiv preprint arXiv:2501.06582", "url": "https://arxiv.org/abs/2501.06582", "problem": "Information retrieval is foundational to contract drafting, but lacks expert-annotated retrieval benchmarks for contract clause retrieval", "prior_assumptions": "General IR benchmarks sufficient for legal contract retrieval tasks", "insight": "Contract clause retrieval requires specialized evaluation with expert relevance judgments on complex clauses", "technical_approach": "114 queries with 126,000+ query-clause pairs rated 1-5 stars by experts, focusing on complex clauses (Limitation of Liability, Indemnification, Change of Control, Most Favored Nation)", "evaluation": "Bi-encoder retrievers with pointwise LLM re-rankers on expert-annotated relevance judgments", "impact": "First expert-annotated contract clause retrieval benchmark for legal IR research", "relevance_to_our_work": "Validates expert annotation approach and contract-specific evaluation methodology", "assumptions_made": ["Expert annotation necessary for legal IR evaluation", "Complex contract clauses most important for evaluation", "Star-rating scale adequate for relevance"], "bit_flip_potential": "Challenges assumption that general IR adequate for legal domains - demonstrates need for expert-annotated contract-specific benchmarks"}

{"title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text", "authors": ["Yunhan Li", "Gengshen Wu"], "year": 2025, "venue": "arXiv preprint arXiv:2505.24826", "url": "https://arxiv.org/abs/2505.24826", "problem": "Current legal AI evaluation focuses on factual accuracy while neglecting linguistic quality aspects like clarity, coherence, and terminology", "prior_assumptions": "Factual accuracy sufficient metric for legal text evaluation", "insight": "Legal text quality requires multi-dimensional assessment including clarity, coherence, and terminology", "technical_approach": "Regression model evaluating quality across three dimensions, tested on 49 LLMs with statistical significance testing", "evaluation": "Comprehensive analysis revealing quality plateau at 14B parameters, minimal impact of engineering choices, reasoning models outperform base architectures", "impact": "First comprehensive quality-focused legal text evaluation framework with cost-performance analysis", "relevance_to_our_work": "Supports our multi-dimensional evaluation approach emphasizing clarity and accessibility", "assumptions_made": ["Quality can be decomposed into clarity, coherence, terminology", "Regression models can capture expert quality judgments", "Legal text quality distinct from general domain"], "bit_flip_potential": "Challenges assumption that accuracy sufficient for legal AI - demonstrates need for holistic quality assessment"}

{"title": "oab-bench: Brazilian Bar Exam Legal Writing Evaluation", "authors": ["Ramon Pires", "et al."], "year": 2025, "venue": "arXiv preprint arXiv:2504.21202", "url": "https://arxiv.org/abs/2504.21202", "problem": "Lack of benchmarks for evaluating legal writing with comprehensive evaluation guidelines", "prior_assumptions": "General writing evaluation sufficient for legal domain assessment", "insight": "Legal writing requires specialized evaluation incorporating domain expertise and structured guidelines", "technical_approach": "105 questions across seven law areas with comprehensive grading guidelines from human examiners, LLM-as-judge evaluation", "evaluation": "Strong correlation with human scores for approved exams, Claude-3.5 Sonnet achieves 7.93/10 average", "impact": "First comprehensive legal writing benchmark with validated automated evaluation", "relevance_to_our_work": "Demonstrates LLM judge effectiveness for legal text quality assessment", "assumptions_made": ["Legal writing evaluation can be automated with proper guidelines", "Bar exam questions representative of legal writing tasks", "LLM judges can replace human evaluation with proper calibration"], "bit_flip_potential": "Challenges assumption that human evaluation only reliable method for legal writing - shows LLM judges can be calibrated for domain-specific assessment"}

{"title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "authors": ["Michael FÃ¤rber", "et al."], "year": 2025, "venue": "arXiv preprint arXiv:2504.14223", "url": "https://arxiv.org/abs/2504.14223", "problem": "Limited availability of simplified materials creates barriers to comprehension for diverse audiences", "prior_assumptions": "General text simplification adequate for legal and specialized content", "insight": "Plain language practices require tailored customization for different target groups and simplicity levels", "technical_approach": "LLM-based system (GPT-4, Llama-3) with flexible customization for various audiences and input formats", "evaluation": "Multi-metric evaluation across different target groups and simplification levels", "impact": "First comprehensive plain language system leveraging LLMs for inclusive content creation", "relevance_to_our_work": "Demonstrates LLM capabilities for plain language conversion aligned with our accessibility goals", "assumptions_made": ["LLMs can be effectively prompted for plain language conversion", "Customization for target audiences improves accessibility", "Multiple input formats necessary for practical systems"], "bit_flip_potential": "Challenges assumption that one-size-fits-all simplification adequate - demonstrates need for audience-specific plain language approaches"}

{"title": "Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law", "authors": ["Sareh Nabi", "et al."], "year": 2025, "venue": "arXiv preprint arXiv:2502.17638", "url": "https://arxiv.org/abs/2502.17638", "problem": "Legal services require higher accuracy, repeatability, and transparency than current LLMs provide", "prior_assumptions": "LLMs alone sufficient for legal reasoning tasks", "insight": "Neuro-symbolic approach integrating LLMs with logic-based reasoning addresses limitations in legal contract analysis", "technical_approach": "Three methodologies: vanilla LLM, unguided LLM+Logic encoding, guided LLM+Logic with framework for contract encoding", "evaluation": "Applied to insurance contract coverage queries, guided approach shows promising capabilities", "impact": "Demonstrates potential for hybrid approaches in legal reasoning with improved accuracy and transparency", "relevance_to_our_work": "Shows importance of structured approaches to legal document analysis and the potential for hybrid methodologies", "assumptions_made": ["Logic programs can encode legal concepts as structured rules", "LLM+Logic combination improves accuracy over pure LLM approaches", "Guided frameworks improve encoding quality"], "bit_flip_potential": "Challenges assumption that end-to-end LLMs optimal for legal reasoning - shows need for structured hybrid approaches"}