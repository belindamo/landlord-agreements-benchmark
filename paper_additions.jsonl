{"id": "wang2025_acord", "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "authors": ["Steven H. Wang", "Maksim Zubkov", "Kexin Fan", "Sarah Harrell", "Yuyang Sun", "Wei Chen", "Andreas Plesner", "Roger Wattenhofer"], "year": 2025, "venue": "arXiv:2501.06582", "url": "https://arxiv.org/abs/2501.06582", "problem": "Contract clause retrieval for legal drafting requires understanding precedent quality beyond text similarity", "prior_assumption": "Legal information retrieval could be handled by general-purpose retrieval systems without domain-specific annotation", "insight": "Contract clause retrieval requires understanding legal precedent quality and contextual relevance beyond text similarity", "technical_approach": "114 queries covering complex clauses with 126,000+ query-clause pairs, 5-star expert relevance ratings, bi-encoder retrievers + pointwise LLM re-rankers", "evaluation": "Standard IR metrics (NDCG, MAP) adapted for legal precedent quality, multiple expert annotators with inter-annotator agreement", "impact": "First retrieval benchmark for contract drafting with full expert annotation, establishes contract drafting as distinct IR research area", "relevance_to_our_work": "Validates expert annotation approach for legal AI benchmarks, demonstrates importance of domain-specific legal evaluation", "assumptions_identified": ["Expert annotation is essential for legal IR benchmarks", "Legal precedent quality matters beyond textual similarity", "Contract drafting workflow requires specialized retrieval systems"], "potential_bit_flips": ["Legal IR should focus on consumer-facing tools vs lawyer productivity", "Quality measured by end-user comprehension vs technical accuracy"]}
{"id": "sancheti2023_party_specific", "title": "What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions", "authors": ["Abhilasha Sancheti", "Aparna Garimella", "Balaji Vasan Srinivasan", "Rachel Rudinger"], "year": 2023, "venue": "arXiv:2212.09825", "url": "https://arxiv.org/abs/2212.09825", "problem": "Contract comprehension is tedious due to length and domain-specificity; different parties need to understand different rights/duties", "prior_assumption": "Generic contract summarization serves all parties equally well", "insight": "Party-specific extractive summarization recognizing that contract importance varies by contracting party", "technical_approach": "293K sentence pairs from lease agreements with expert pairwise importance annotations, content categorizer + importance ranker pipeline", "evaluation": "ROUGE scores, human preference judgments, expert annotation quality assessment", "impact": "Largest expert-annotated dataset for legal contract importance ranking, establishes party-specific legal summarization", "relevance_to_our_work": "Closest prior work to consumer-focused approach, validates expert annotation methodology for legal comprehension", "assumptions_identified": ["Party-specific perspective is crucial for legal document summarization", "Expert annotation can capture legal importance", "Pairwise comparison more reliable than absolute rating"], "potential_bit_flips": ["Contract summarization should maintain legal language vs translate to plain English", "Serve contracting parties vs general consumers", "Extractive summarization vs abstractive explanation"]}
{"id": "logicLease2025_rental", "title": "Prolog and LLMs for Rental Law Compliance in New York", "authors": ["Authors not clearly listed"], "year": 2025, "venue": "arXiv:2502.09204", "url": "https://arxiv.org/abs/2502.09204", "problem": "Automating analysis of landlord-tenant legal cases requires transparent logical reasoning combined with natural language processing", "prior_assumption": "LLMs alone can handle legal reasoning tasks effectively", "insight": "Hybrid approach separating information extraction (LLMs) from legal reasoning (Prolog) for transparency", "technical_approach": "LogicLease system: LLMs extract information, Prolog handles legal logic, 100% accuracy, 2.57 second processing", "evaluation": "Accuracy testing on landlord-tenant legal cases with ground truth compliance determinations", "impact": "Demonstrates hybrid AI approaches achieve high accuracy with transparency for legal reasoning", "relevance_to_our_work": "Direct landlord-tenant domain overlap, emphasis on system transparency and explainability", "assumptions_identified": ["Hybrid approaches outperform pure LLM solutions", "Transparency is essential for legal AI", "Separation of information extraction and reasoning improves reliability"], "potential_bit_flips": ["Legal AI should focus on professional analysis vs consumer understanding", "Formal logical programming vs natural language explanation", "Accuracy as primary measure vs comprehensibility equally important"]}
{"id": "pires2025_legal_writing", "title": "Automatic Legal Writing Evaluation of LLMs", "authors": ["Ramon Pires"], "year": 2025, "venue": "arXiv:2504.21202", "url": "https://arxiv.org/abs/2504.21202", "problem": "Legal writing evaluation requires domain-specific benchmarks with comprehensive guidelines", "prior_assumption": "General language benchmarks can adequately assess legal writing capabilities", "insight": "LLM judges (OpenAI o1) achieve strong correlation with human legal experts when properly calibrated", "technical_approach": "oab-bench with 105 questions across seven law areas, comprehensive evaluation guidelines, Claude-3.5 Sonnet evaluation", "evaluation": "Model performance on legal writing tasks, LLM judge correlation with human scores", "impact": "Validates LLM-as-judge approach for legal domain evaluation, provides calibration methodology", "relevance_to_our_work": "Validates LLM-as-judge approach for legal domain evaluation, provides methodology for calibrating legal judges", "assumptions_identified": ["Domain-specific evaluation guidelines essential", "LLM judges can achieve reliable correlation with legal experts", "Bar examination materials provide appropriate benchmarking"], "potential_bit_flips": ["Legal evaluation should focus on professional competency vs consumer comprehension", "Legal writing evaluation vs plain language quality assessment", "Bar exam standards vs consumer understanding standards"]}