{"title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "year": 2025, "venue": "ArXiv", "arxiv_id": "2508.03080", "problem": "Need systematic evaluation of whether open-source LLMs could match proprietary LLMs in identifying clause-level legal risks in commercial contracts", "assumption_prior_work": "General benchmarks sufficient; limited comparison between proprietary and open-source models", "insight": "Multi-dimensional evaluation reveals proprietary models outperform open-source in correctness, but most LLMs perform at junior legal assistant level", "technical_approach": "Evaluated 4 proprietary and 15 open-source LLMs on CUAD dataset across five dimensions: correctness vs effectiveness, model size effects, reasoning modes, response frequency, quantization impacts", "evaluation": "Comprehensive evaluation on CUAD showing proprietary superiority, diminishing returns with scale, reasoning mode trade-offs", "impact": "First comprehensive proprietary vs open-source legal benchmark, practical deployment guidance, demonstrates need for targeted fine-tuning", "relevance": "Validates domain-specific benchmarking approach and multi-dimensional evaluation methodology", "category": "legal_benchmarking"}
{"title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr√ºwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "year": 2025, "venue": "ArXiv", "arxiv_id": "2505.12864", "problem": "Long-form legal reasoning remains challenging for LLMs despite advances in test-time scaling", "assumption_prior_work": "General reasoning benchmarks sufficient to measure legal reasoning capabilities", "insight": "Legal reasoning requires comprehensive assessment with explicit reasoning guidance (issue spotting, rule recall, rule application) that mirrors actual legal education", "technical_approach": "Compiled 340 law exams, 4,886 questions in English/German, explicit reasoning guidance, LLM-as-Judge with human validation", "evaluation": "Comprehensive evaluation reveals LLM struggles with structured multi-step reasoning, effective model differentiation", "impact": "Most comprehensive legal reasoning benchmark, establishes gold standard for evaluation, validates LLM-as-Judge framework", "relevance": "Establishes methodology for legal reasoning evaluation, validates our LLM-as-Judge approach", "category": "legal_benchmarking"}
{"title": "LogicLease: Prolog and LLMs for Rental Law Compliance in New York", "authors": ["Sanskar Sehgal", "Yanhong A. Liu"], "year": 2025, "venue": "ArXiv", "arxiv_id": "2502.09204", "problem": "Landlord-tenant legal cases need automated analysis combining transparent reasoning with natural language accessibility", "assumption_prior_work": "Either pure LLM or rule-based systems alone sufficient for legal analysis", "insight": "Neuro-symbolic approach (LLM + Prolog) separates information extraction from legal reasoning, achieving transparency while avoiding hallucinations", "technical_approach": "Combined LLM information extraction with Prolog legal reasoning for NY landlord-tenant cases, transparent step-by-step reasoning with legal citations", "evaluation": "100% accuracy in 2.57 seconds per case, robust across case scenarios, avoided LLM hallucinations", "impact": "Demonstrates viability of neuro-symbolic legal AI, provides transparent explainable reasoning, establishes framework for reliable compliance analysis", "relevance": "Directly addresses landlord-tenant domain but focuses on legal compliance rather than consumer comprehension - validates our bit flip", "category": "domain_specific_legal"}
{"title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements", "authors": ["Giorgos Filandrianos"], "year": 2025, "venue": "ArXiv", "arxiv_id": "2506.00608", "problem": "Contract review is complex, time-intensive, requires expertise; contracts are confidential limiting proprietary model use", "assumption_prior_work": "Single-model approaches or proprietary systems sufficient for contract analysis", "insight": "Multi-agent collaborative workflows with privacy-preserving RAG enable accessible, adaptable legal document review", "technical_approach": "Fully open-source multi-agent framework with plug-and-play architecture, collaborative agent workflows, novel RAG for legal documents", "evaluation": "Outperformed single models in accuracy, retrieval, explainability, completeness through human study and automated metrics", "impact": "Establishes viability of multi-agent legal AI, provides privacy-preserving alternative, demonstrates collaborative agent benefits", "relevance": "Validates multi-agent approaches and privacy-preserving design for legal document analysis", "category": "legal_systems"}
{"title": "LegalEval-Q: A New Benchmark for Quality Evaluation of LLM-Generated Legal Text", "authors": ["Yunhan Li"], "year": 2025, "venue": "ArXiv", "arxiv_id": "2505.24826", "problem": "Current evaluation focuses on factual accuracy while neglecting linguistic quality (clarity, coherence, terminology)", "assumption_prior_work": "Factual accuracy is primary measure of legal AI quality", "insight": "Legal text quality requires multi-dimensional evaluation; model performance plateaus at 14B parameters", "technical_approach": "Regression model for quality evaluation across clarity, coherence, terminology; analyzed 49 LLMs with specialized legal questions", "evaluation": "Revealed quality plateau at 14B parameters, negligible impact of engineering choices, superiority of reasoning models", "impact": "First standardized quality evaluation for legal LLMs, reveals training limitations, provides cost-performance guidance", "relevance": "Critical for our plain language evaluation methodology, validates focus on clarity and coherence", "category": "legal_evaluation"}