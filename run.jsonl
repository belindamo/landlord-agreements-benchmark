{"id":"run_1702600000001","experimentId":"exp_1702400000000","title":"DistilBERT vs TinyBERT Baseline","status":"running","startDate":"2024-01-25T09:00:00Z","endDate":"","codeUrl":"experiments/efficient_transformer_001/run/baseline.py","dataPath":"data/glue_benchmark/","hyperparameters":{"models":["bert-base","distilbert","tinybert"],"batch_size":32,"eval_batch_size":64,"device":"A100"},"metrics":{"progress":"60%","glue_scores":{"bert-base":{"cola":52.1,"sst2":92.5,"mrpc":88.9},"distilbert":{"cola":48.2,"sst2":91.3,"mrpc":86.5}}},"notes":"Running baseline comparison. TinyBERT evaluation pending. GPU memory usage significantly lower for distilled models.","createdDate":"2024-01-25T08:30:00Z"}